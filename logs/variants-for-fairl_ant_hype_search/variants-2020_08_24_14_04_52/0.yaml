adv_irl_params:
  disc_lr: 0.0003
  disc_momentum: 0.0
  disc_optim_batch_size: 256
  eval_deterministic: true
  freq_saving: 20
  grad_pen_weight: 64.0
  max_path_length: 1000
  min_steps_before_training: 5000
  mode: airl
  no_terminal: true
  num_disc_updates_per_loop_iter: 1
  num_epochs: 162
  num_policy_updates_per_loop_iter: 1
  num_steps_between_train_calls: 1000
  num_steps_per_epoch: 100000
  num_steps_per_eval: 20000
  num_update_loops_per_train_call: 100
  policy_optim_batch_size: 256
  policy_optim_batch_size_from_expert: 0
  replay_buffer_size: 20000
  save_algorithm: false
  save_best: true
  save_environment: false
  save_replay_buffer: false
  state_only: false
  use_grad_pen: true
  wrap_absorbing: false
description: Train an adversarial IRL model
disc_clamp_magnitude: 10.0
disc_hid_act: tanh
disc_hid_dim: 128
disc_num_blocks: 2
disc_use_bn: false
env_specs:
  env_kwargs: {}
  env_name: halfcheetah
  eval_env_seed: 78236
  training_env_seed: 24495
exp_id: 0
exp_name: fairl_ant_hype_search
expert_idx: 0
expert_name: norm_halfcheetah_16_demos_sub_20
mem_per_worker: 4gb
node_exclusions: gpu048,gpu024,gpu025,gpu012,gpu027
num_cpu_per_worker: 32
num_gpu_per_worker: 1
num_workers: 1
partitions: cpu
policy_net_size: 256
policy_num_hidden_layers: 2
sac_params:
  beta_1: 0.25
  discount: 0.99
  policy_lr: 0.0003
  policy_mean_reg_weight: 0.001
  policy_std_reg_weight: 0.001
  qf_lr: 0.0003
  reward_scale: 2.0
  soft_target_tau: 0.005
  vf_lr: 0.0003
scale_env_with_demo_stats: true
script_path: run_scripts/adv_irl_exp_script.py
seed: 723894
