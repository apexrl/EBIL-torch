meta_data:
  script_path: run_scripts/ebil_exp_script.py
  exp_name: ebil_deen_walker_4_hype_search
  description: Train an EBIL model in Walker with DEEN
  num_workers: 5 # 64
  num_gpu_per_worker: 1 # 1 
  num_cpu_per_worker: 4 # 2
  mem_per_worker: 4gb
  partitions: cpu
  node_exclusions: gpu048,gpu024,gpu025,gpu012,gpu027
# -----------------------------------------------------------------------------
variables:
  ebm_epoch: [100000] # [500, 1000, 1500, 2000, 2500, 3000,'best']
  ebm_sigma:  [0.01, 0.05, 0.1]
  sac_params:
    reward_scale: [64., 32.]
  # seed: [723894, 23789]
  seed: [723894]

# -----------------------------------------------------------------------------
constants:
  test: false # true
  expert_name: 'norm_walker_4_demos_sub_20'
  expert_traj_num: 4
  expert_idx: 0
  scale_env_with_demo_stats: true
  minmax_env_with_demo_stats: false

  ebm_num_blocks: 5
  ebm_hid_dim: 512
  ebm_hid_act: tanh
  ebm_use_bn: false

  policy_net_size: 256
  policy_num_hidden_layers: 2

  rew_func: "-energy+1"
  cons: 0

  pretrain: false

  ebil_params:
    mode: "deen"
    state_only: false
    clamp_magnitude: -1

    num_epochs: 362
    num_steps_per_epoch: 10000
    num_steps_between_train_calls: 1000
    max_path_length: 1000
    min_steps_before_training: 5000

    eval_deterministic: true
    num_steps_per_eval: 20000
    
    replay_buffer_size: 1000000
    no_terminal: true
    wrap_absorbing: false

    num_policy_update_loops_per_train_call: 100
    num_policy_updates_per_loop_iter: 1

    use_grad_pen: false
    # grad_pen_weight: 10.0
    policy_optim_batch_size: 256
    policy_optim_batch_size_from_expert: 0

    save_best: true
    freq_saving: 10
    save_replay_buffer: false
    save_environment: false
    save_algorithm: false

  bc_params:
    mode: 'MLE'

    num_epochs: 10
    num_steps_per_epoch: 1000
    num_steps_between_train_calls: 1000
    max_path_length: 1000
    min_steps_before_training: 0

    eval_deterministic: true
    num_steps_per_eval: 10000
    
    no_terminal: true
    wrap_absorbing: false

    num_updates_per_train_call: 1
    lr: 0.0003
    momentum: 0.9
    batch_size: 256

    save_best: true
    save_best_starting_from_epoch: 0

    freq_saving: 20
    save_replay_buffer: false
    save_environment: false
    save_algorithm: false

  sac_params:
    # reward_scale: 8.0
    discount: 0.99
    soft_target_tau: 0.005
    beta_1: 0.25
    policy_lr: 0.0003
    qf_lr: 0.0003
    vf_lr: 0.0003
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001

  env_specs:
    env_name: 'walker'
    env_kwargs: {}
    eval_env_seed: 78236
    training_env_seed: 24495
